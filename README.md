# LM-GAN

Literature review 


Papers
1. C-RNN-GAN Continuous recurrent neural networks with adversarial training https://arxiv.org/abs/1611.09904
code: https://github.com/olofmogren/c-rnn-gan
2. TUNING RECURRENT NEURAL NETWORKS WITH REINFORCEMENT LEARNING https://arxiv.org/pdf/1611.02796v2.pdf                     code https://github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner
3. Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control https://dam-prod.media.mit.edu/x/2017/02/27/icml-2017-sequence%20%284%29_Tbtdy1q.pdf   
4. SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient https://arxiv.org/pdf/1609.05473.pdf                   code https://github.com/LantaoYu/SeqGAN
5. sequence-gan code https://github.com/ofirnachum/sequence_gan

For Discrete distributions 
1. GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution https://arxiv.org/pdf/1611.04051.pdf
2. Boundary-Seeking Generative Adversarial Networks https://arxiv.org/abs/1702.08431
3. Maximum-Likelihood Augmented Discrete Generative Adversarial Networks https://arxiv.org/pdf/1702.07983v1.pdf


Code and Tutorials 
1. tutorial series on RL using TF - https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0

Forums/Community 
1. OpenAI Gym - https://gitter.im/openai/gym
2. Pytorch - https://gitter.im/torch/torch7

==================================================================================================
For English Parse Rules 
1. You might start by pulling out the most common productions from the Penn Treebank or similar corpus.  Here is a little gist to do that in NLTK:  https://gist.github.com/leebecker/95bc468d486878ef300ff1ba82652ccd
2. Treebanking guidelines for parse rules https://clear.colorado.edu/compsem/documents/treebank_guidelines.pdf http://cs.jhu.edu/~jason/465/hw-parse/treebank-manual.pdf

